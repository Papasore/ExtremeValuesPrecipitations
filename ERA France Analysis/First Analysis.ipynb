{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d64f41b-ac1c-44a6-85b6-c8be29fc03cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NEED TO BE RUN TWICE (Unknown error)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "import scipy.stats as stats\n",
    "import xarray as xr\n",
    "import zipfile\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import scipy.stats as stats\n",
    "from scipy.optimize import minimize\n",
    "import plotly.graph_objs as go\n",
    "import pyreadr\n",
    "import math\n",
    "import time\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import plotly.express as px\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import s3fs\n",
    "# Create filesystem object\n",
    "S3_ENDPOINT_URL = \"https://\" + os.environ[\"AWS_S3_ENDPOINT\"]\n",
    "fs = s3fs.S3FileSystem(client_kwargs={'endpoint_url': S3_ENDPOINT_URL})\n",
    "BUCKET = \"zhippofficiel\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58289a75-988b-4fbd-89b0-70c9d8f4339c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical models and function for modelization of precipitations.\n",
    "# Include GEV and EGPD functions.\n",
    "\n",
    "def H(xi,X):\n",
    "    X=np.array(X)\n",
    "    return((1+xi*X)**(-1/xi))\n",
    "\n",
    "def F(params,X):\n",
    "    kappa,sigma,xi = params\n",
    "    X=np.array(X)\n",
    "    return(H(xi,X/sigma)**kappa)\n",
    "\n",
    "def f(params,X):\n",
    "    kappa,sigma,xi = params\n",
    "    return((kappa/sigma)*((1+((xi/sigma)*X))**(-1-(kappa/xi))))\n",
    "\n",
    "def minus_l_f(params,X):\n",
    "    kappa,sigma,xi = params\n",
    "    return -np.sum(list(np.log((kappa/sigma)*((1+((xi/sigma)*X))**(-1-(kappa/xi))))))\n",
    "\n",
    "def H_inv(xi,X):\n",
    "    X=np.array(X)\n",
    "    return(((X**(-xi))-1)/xi)\n",
    "\n",
    "def Q(params,p):\n",
    "    kappa,sigma,xi = params\n",
    "    p = np.array(p)\n",
    "    return((sigma/xi)*((p**(-xi/kappa))-1))\n",
    "\n",
    "def data_xhour(data,x):\n",
    "    results = []\n",
    "    N=len(data)\n",
    "    for i in range(N//x):\n",
    "        value = 0\n",
    "        for j in range(x):\n",
    "            value = value + data[i*x+j]\n",
    "        results.append(value)\n",
    "    return results\n",
    "\n",
    "def get_optimal_params(lat,lon,duration,treshold): #lat,lon in '41.25' format; duration in hour (only int) ; treshold in m\n",
    "    selected_value = list(full_dataset.sel(lon=lon, lat=lat).values)\n",
    "    list_precip = np.array(selected_value)\n",
    "    data_dur = np.array(data_xhour(list_precip,duration))\n",
    "    cleaned_data = np.array([(x-treshold)*1000 for x in data_dur if x>treshold])\n",
    "    initial_guess = [0.5, 1, 0.3]\n",
    "    objective_function = lambda params: minus_l_f(params, cleaned_data)\n",
    "    result_minimize = minimize(objective_function, initial_guess, method='Nelder-Mead')\n",
    "    optimal_params = result_minimize.x\n",
    "    return optimal_params\n",
    "\n",
    "def file_oneD_all_points(duration,treshold):\n",
    "    Store_params = []\n",
    "    lons = list(full_dataset['lon'].values)\n",
    "    lats = list(full_dataset['lat'].values)\n",
    "    for lat in lats:\n",
    "        print('Starting to work on points with latitude {}°N'.format(float(lat)))\n",
    "        for lon in lons:\n",
    "            optimal_params  = get_optimal_params(lat,lon,duration,treshold)\n",
    "            res = list(optimal_params)\n",
    "            res.append(lat)\n",
    "            res.append(lon)\n",
    "            Store_params.append(res)\n",
    "    return Store_params\n",
    "\n",
    "def create_save_optimal_params(durations,treshold): #durations in hours ([int] only) ; treshold in m \n",
    "    FILEDIR_KEY_OUT_S3 = \"ExtremeValuesPrecipitations/ERA Analysis/Optimal_parameters\"       \n",
    "    print(\" ------------------------- Starting searching optimal parameters. ------------------------- \")\n",
    "    for duration in durations:\n",
    "        start_time = time.time()\n",
    "        print(\" ------------------------- For a period of {} hours. ------------------------- \".format(duration))\n",
    "        FILE_KEY_OUT_S3 = FILEDIR_KEY_OUT_S3+\"/optimal_params_\"+str(duration)+\"_hours.txt\"\n",
    "        optimal_params_oneD = file_oneD_all_points(duration,treshold)\n",
    "        optimal_params_oneD_list=[]\n",
    "        for x in optimal_params_oneD:\n",
    "            value = list(x)\n",
    "            optimal_params_oneD_list.append(value)\n",
    "            column_names = ['kappa', 'sigma', 'xi','lat','lon']\n",
    "        Optimal_params_df = pd.DataFrame(optimal_params_oneD_list, columns=column_names)\n",
    "        FILE_PATH_OUT_S3 = BUCKET_OUT + \"/\" + FILE_KEY_OUT_S3\n",
    "        with fs.open(FILE_PATH_OUT_S3, 'w') as file_out:\n",
    "            Optimal_params_df.to_csv(file_out, sep='\\t', index=False)\n",
    "        print(' --- Saving File Completed. ---')\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        print(\"Time taken to compute data: {:.2f} seconds\".format(elapsed_time))\n",
    "    return(\"Task completed successfully!\")\n",
    "\n",
    "def return_levels_to_quantiles(return_levels,duration): # return_level en années ; duration en heures \n",
    "    return_levels = np.array(return_levels)\n",
    "    res = (np.array([(duration/(365*time*24)) for time in return_levels]))\n",
    "    return res\n",
    "\n",
    "def IDF_one_D(params,return_levels,duration):\n",
    "    quantiles = return_levels_to_quantiles(return_levels,duration)\n",
    "    Qvalues = Q(params,quantiles)\n",
    "    return Qvalues\n",
    "\n",
    "def get_return_levels_int_or_cumul(lat,lon,durations,return_levels,Int):\n",
    "    FILEDIR_KEY_S3 = \"ExtremeValuesPrecipitations/ERA Analysis/Optimal_parameters\"      \n",
    "    Return_Values = [] \n",
    "    for dur in durations:\n",
    "        FILE_KEY_S3 = FILEDIR_KEY_S3 + '/optimal_params_'+str(dur)+'_hours.txt'\n",
    "        FILE_PATH_S3 = BUCKET + \"/\" + FILE_KEY_S3\n",
    "        with fs.open(FILE_PATH_S3, mode=\"rb\") as file_in:\n",
    "            df = pd.read_csv(file_in, sep='\\t')\n",
    "        selected_point = df[(df['lat'] == lat) & (df['lon'] == lon)]\n",
    "        optimal_params = [float(selected_point['kappa']),float(selected_point['sigma']),float(selected_point['xi'])]\n",
    "        Qvalues = IDF_one_D(optimal_params,return_levels,dur)\n",
    "        Return_Values.append(Qvalues)\n",
    "    plt.figure()\n",
    "    for i in range(len(return_levels)):\n",
    "        values = []\n",
    "        for j in range(len(durations)):\n",
    "            if Int:\n",
    "                values.append(Return_Values[j][i]/durations[j])\n",
    "            if not Int:\n",
    "                values.append(Return_Values[j][i])\n",
    "        plt.plot(durations,values,marker='s',label='Return levels over {} Year'.format(return_levels[i]))\n",
    "    plt.xticks(durations)\n",
    "    plt.xlim(0.6,15)\n",
    "    #plt.xscale('log')\n",
    "    #plt.yscale('log')\n",
    "    plt.grid()\n",
    "    #plt.legend()\n",
    "    plt.xlabel('Durations (hours)')\n",
    "    if Int:\n",
    "        plt.ylabel('Intensity (mm/hour)')\n",
    "    if not Int:\n",
    "        plt.ylabel('Cumuls (mm)')\n",
    "    plt.title('IDF Curve')\n",
    "    plt.show()\n",
    "    return Return_Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6b0a1d-281a-45cc-95ba-646cea4ab2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading ERA data from database\n",
    "\n",
    "FILEDIR_KEY_S3 = \"ExtremeValuesPrecipitations/ERA DATA\"\n",
    "file_name1 = BUCKET + \"/\" + FILEDIR_KEY_S3 + '/test.Rdata'\n",
    "with fs.open(file_name1, mode=\"rb\") as file_in:\n",
    "    ds1 = pyreadr.read_r(file_in)\n",
    "dataframe1 = ds1['A']\n",
    "\n",
    "file_name2 = BUCKET + \"/\" + FILEDIR_KEY_S3 +'/test2.Rdata'\n",
    "with fs.open(file_name1, mode=\"rb\") as file_in:\n",
    "    ds2 = pyreadr.read_r(file_in)\n",
    "dataframe2 = ds2['A']\n",
    "file_name3 = BUCKET + \"/\" + FILEDIR_KEY_S3 +'/test3.Rdata'\n",
    "with fs.open(file_name1, mode=\"rb\") as file_in:\n",
    "    ds3 = pyreadr.read_r(file_in)\n",
    "dataframe3 = ds3['A']\n",
    "file_name4 = BUCKET + \"/\" + FILEDIR_KEY_S3 +'/test4.Rdata'\n",
    "with fs.open(file_name1, mode=\"rb\") as file_in:\n",
    "    ds4 = pyreadr.read_r(file_in)\n",
    "dataframe4 = ds4['A']\n",
    "\n",
    "datasets = [dataframe1,dataframe2,dataframe3,dataframe4]\n",
    "concatenated_dataset = xr.concat(datasets, dim='dim_2')\n",
    "new_names = {'dim_0': 'lon', 'dim_1': 'lat', 'dim_2': 'date'}\n",
    "full_dataset = concatenated_dataset.rename(new_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
